# FoNE Pretraining Configuration
# LLaMA 1.5B with FoNE number embeddings

model_config: configs/llama_1p5b.json
data_config: configs/data_local_1p5b_1x.json

# Training hyperparameters
per_device_train_batch_size: 6
gradient_accumulation_steps: 4
learning_rate: 3.0e-4
warmup_steps: 3000
weight_decay: 0.1
save_every_steps: 10000
log_every_steps: 10

# Distributed training
num_processes: 8
mixed_precision: bf16
dataloader_num_workers: 0

# Wandb logging
wandb_project: fone-pretraining
wandb_run_name: fone_1p5b

# HuggingFace upload (checkpoints will be uploaded here and cleaned up locally)
hf_repo_id: Onlydrinkwater/fone-1p5b

# FoNE specific: null or 999 enables FoNE (Fourier embeddings for numbers 0-999)
# Set to -1 to disable FoNE (baseline mode)
fone_hi: null

